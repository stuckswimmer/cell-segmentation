{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torchvision Train Harness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch \n",
    "import torchvision\n",
    "from torchvision.transforms import ToPILImage\n",
    "from torchvision.transforms import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix alll random seeds for repeatability\n",
    "def fix_all_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_all_seeds(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "\n",
    "Configuration for the model including set up dataset loader, neural net configuration, class dictionary.\n",
    "\n",
    "Input parameters for conversion into software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = False\n",
    "data_directory = 'data'\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "BATCH_SIZE = 1\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "TRAIN_CSV = 'data/train.csv'\n",
    "TRAIN = 'data/train'\n",
    "TEST = 'data/test'\n",
    "\n",
    "WIDTH = 704\n",
    "HEIGHT = 520\n",
    "\n",
    "resize_factor = False\n",
    "\n",
    "NORMALIZE = False\n",
    "RESNET_MEAN = (0.485, 0.456, 0.406)\n",
    "RESNET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "MOMENTUM = 0.9\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.0005\n",
    "\n",
    "#Tweaking specific confidence levels for each class\n",
    "cell_type_dictionary = {'astro':1, 'cort': 2, 'shsy5y':3}\n",
    "mask_threshold_dict = {1: 0.55, 2: 0.75, 3: 0.6}\n",
    "min_score_dict = {1: 0.55, 2: 0.75, 3: 0.5}\n",
    "\n",
    "USE_SCHEDULER = False\n",
    "PCT_IMAGES_VALIDATION = 0.075\n",
    "BOX_DETECTIONS_PER_IMG = 540\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions\n",
    "\n",
    "Functions involved for translating annotations into usable masks from run length encoding into image masks for training segmentation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See input_data_notebook for code testing for extraction; that code used the pycocotools api\n",
    "\n",
    "def rle_decode(mask_rle, shape, color = 1):\n",
    "    \"\"\"\n",
    "    mask_rle: mask as run length encoded fomatted string (start length)\n",
    "    shape: (height, width, channels) of resulting image array\n",
    "    color: color return for binary mask\n",
    "    returns mask as numpy array \n",
    "    \"\"\"\n",
    "    s = mask_rle.split()\n",
    "    starts = list(map(lambda x: int(x) - 1, s[0::2]))\n",
    "    lengths = list(map(int,s[1::2]))\n",
    "    ends = [x + y for x,y in zip(starts, lengths)]\n",
    "    \n",
    "    if len(shape) == 3:\n",
    "        img = np.zeros((shape[0] * shape[1], shape[2]), dtype=np.float32)\n",
    "    else:\n",
    "        img = np.zeros(shape[0] * shape[1], dtype=np.float32)\n",
    "    \n",
    "    for start, end in zip(starts, ends):\n",
    "        img[start:end] = color\n",
    "    \n",
    "    return img.reshape(shape)\n",
    "\n",
    "def rle_encoding(x):\n",
    "    dots = np.where(x.flatten()== 1)[0]\n",
    "    run_lengths = []\n",
    "    prev =-2\n",
    "    for dot in dots:\n",
    "        if (dot > prev +1):\n",
    "            run_lengths.extend((dot + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = dot\n",
    "    return ' '.join(map(str, run_lengths))\n",
    "\n",
    "def remove_overlapping_pixels(mask, other_masks):\n",
    "    for other_mask in other_mask:\n",
    "        if np.sum(np.logical_and(mask, other_mask)) > 0:\n",
    "            mask[np.logical_and(mask, other_mask)] = 0\n",
    "    return mask\n",
    "\n",
    "def combine_masks(masks, mask_threshold):\n",
    "    \"\"\"combine multiple masks into a single image\"\"\"\n",
    "    maskImg = np.zeros(HEIGHT, WIDTH)\n",
    "    print(len(masks.shape), masks.shape)\n",
    "    for m, mask in enumerate(masks, 1):\n",
    "        maskImg[mask>mask_threshold] = m\n",
    "    return maskImg\n",
    "\n",
    "def get_filtered_masks(pred, min_score_dict, mask_threshold_dict):\n",
    "    \"\"\"filter masks using min_score and Max_threhold for pixels\"\"\"\n",
    "    use_masks = []\n",
    "    for i, mask in enumerate(pred['masks']):\n",
    "        #Filter below threshold scoring results\n",
    "        score = pred['scores'][i].cpu().item()\n",
    "        label = pred['labels'][i].cpu().item()\n",
    "        if score > min_score_dict[label]:\n",
    "            mask = mask.cpu().numpy().squeeze()\n",
    "            # Keep only highly likely pixels\n",
    "            binary_mask = mask > mask_threshold_dict[label]\n",
    "            binary_mask = remove_overlapping_pixels(binary_mask, use_masks)\n",
    "            use_masks.append(binary_mask)\n",
    "    return use_masks\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "This competition data was evaluated on the mean average precision at different intersection over union threshold.\n",
    "\n",
    "This metric sweeps over a range of IOU thresholds at each threshold calculating an average precision value ranging between 0.5 to 0.95 at a step size of 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(labels, y_pred, verbose = 0):\n",
    "    \"\"\"Compute the intersection over union for instance labels and predictions\n",
    "    Args:\n",
    "    labels(numpy array): Labels\n",
    "    Y_pred (numpy array): Predictions\n",
    "\n",
    "    returns:\n",
    "    np array: IoU matrix of size true_objects x prediction_objects\n",
    "    \"\"\"\n",
    "\n",
    "    true_objects = len(np.unique(labels))\n",
    "    pred_objects = len(np.unique(y_pred))\n",
    "    if verbose:\n",
    "        print(\"Number of true objects: {}\".format(true_objects))\n",
    "        print(\"Number of predicted objects: {}\".format(pred_objects))\n",
    "     # Compute intersection between all objects\n",
    "    intersection = np.histogram2d(\n",
    "        labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects)\n",
    "    )[0]\n",
    "\n",
    "    # Compute areas (needed for finding the union between all objects)\n",
    "    area_true = np.histogram(labels, bins=true_objects)[0]\n",
    "    area_pred = np.histogram(y_pred, bins=pred_objects)[0]\n",
    "    area_true = np.expand_dims(area_true, -1)\n",
    "    area_pred = np.expand_dims(area_pred, 0)\n",
    "\n",
    "    # Compute union\n",
    "    union = area_true + area_pred - intersection\n",
    "    intersection = intersection[1:, 1:] # exclude background\n",
    "    union = union[1:, 1:]\n",
    "    union[union == 0] = 1e-9\n",
    "    iou = intersection / union\n",
    "    \n",
    "    return iou  \n",
    "\n",
    "def precision_at(threshold, iou):\n",
    "    \"\"\"\n",
    "    Computes the precision at a given threshold.\n",
    "\n",
    "    Args:\n",
    "        threshold (float): Threshold.\n",
    "        iou (np array): IoU matrix.\n",
    "\n",
    "    Returns:\n",
    "        int: Number of true positives,\n",
    "        int: Number of false positives,\n",
    "        int: Number of false negatives.\n",
    "    \"\"\"\n",
    "    matches = iou > threshold\n",
    "    true_positives = np.sum(matches, axis=1) == 1  # Correct objects\n",
    "    false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n",
    "    false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n",
    "    tp, fp, fn = (\n",
    "        np.sum(true_positives),\n",
    "        np.sum(false_positives),\n",
    "        np.sum(false_negatives),\n",
    "    )\n",
    "    return tp, fp, fn\n",
    "\n",
    "\n",
    "def iou_map(truths, preds, verbose=0):\n",
    "    \"\"\"\n",
    "    Computes the metric for the competition.\n",
    "    Masks contain the segmented pixels where each object has one value associated,\n",
    "    and 0 is the background.\n",
    "\n",
    "    Args:\n",
    "        truths (list of masks): Ground truths.\n",
    "        preds (list of masks): Predictions.\n",
    "        verbose (int, optional): Whether to print infos. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        float: mAP.\n",
    "    \"\"\"\n",
    "    ious = [compute_iou(truth, pred, verbose) for truth, pred in zip(truths, preds)]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n",
    "\n",
    "    prec = []\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        tps, fps, fns = 0, 0, 0\n",
    "        for iou in ious:\n",
    "            tp, fp, fn = precision_at(t, iou)\n",
    "            tps += tp\n",
    "            fps += fp\n",
    "            fns += fn\n",
    "\n",
    "        p = tps / (tps + fps + fns)\n",
    "        prec.append(p)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tps, fps, fns, p))\n",
    "    if verbose:\n",
    "        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n",
    "\n",
    "    return np.mean(prec)\n",
    "\n",
    "def get_score(ds, mdl):\n",
    "    \"\"\"\n",
    "    Get average IOU mAP score for a dataset\n",
    "    \"\"\"\n",
    "    mdl.eval()\n",
    "    iouscore = 0\n",
    "    for i in tqdm(range(len(ds))):\n",
    "        img, targets = ds[i]\n",
    "        with torch.no_grad():\n",
    "            result = mdl([img.to(DEVICE)])[0]\n",
    "            \n",
    "        masks = combine_masks(targets['masks'], 0.5)\n",
    "        labels = pd.Series(result['labels'].cpu().numpy()).value_counts()\n",
    "\n",
    "        mask_threshold = mask_threshold_dict[labels.sort_values().index[-1]]\n",
    "        pred_masks = combine_masks(get_filtered_masks(result), mask_threshold)\n",
    "        iouscore += iou_map([masks],[pred_masks])\n",
    "    return iouscore / len(ds)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations for data augmentation\n",
    "\n",
    "Start with Horizontal and Vertical flip. Because using torchvision model normalisation is included in model pipeline as is any resizing. Transforms are taken from the mask rcnn tutorial in the pytorch documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are slight redefinitions of torch.transformation classes\n",
    "# The difference is that they handle the target and the mask\n",
    "# Copied from Abishek, added new ones\n",
    "class Compose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "class VerticalFlip:\n",
    "    def __init__(self, prob):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.prob:\n",
    "            height, width = image.shape[-2:]\n",
    "            image = image.flip(-2)\n",
    "            bbox = target[\"boxes\"]\n",
    "            bbox[:, [1, 3]] = height - bbox[:, [3, 1]]\n",
    "            target[\"boxes\"] = bbox\n",
    "            target[\"masks\"] = target[\"masks\"].flip(-2)\n",
    "        return image, target\n",
    "\n",
    "class HorizontalFlip:\n",
    "    def __init__(self, prob):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.prob:\n",
    "            height, width = image.shape[-2:]\n",
    "            image = image.flip(-1)\n",
    "            bbox = target[\"boxes\"]\n",
    "            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n",
    "            target[\"boxes\"] = bbox\n",
    "            target[\"masks\"] = target[\"masks\"].flip(-1)\n",
    "        return image, target\n",
    "\n",
    "class Normalize:\n",
    "    def __call__(self, image, target):\n",
    "        image = F.normalize(image, RESNET_MEAN, RESNET_STD)\n",
    "        return image, target\n",
    "\n",
    "class ToTensor:\n",
    "    def __call__(self, image, target):\n",
    "        image = F.to_tensor(image)\n",
    "        return image, target\n",
    "    \n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = [ToTensor()]\n",
    "    if NORMALIZE:\n",
    "        transforms.append(Normalize())\n",
    "    \n",
    "    # Data augmentation for train\n",
    "    if train: \n",
    "        transforms.append(HorizontalFlip(0.5))\n",
    "        transforms.append(VerticalFlip(0.5))\n",
    "\n",
    "    return Compose(transforms)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Custom Datasets and Dataloader\n",
    "\n",
    "A custom Dataset and dataloader class are required for pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_type_dict = {\"astro\": 1, \"cort\": 2, \"shsy5y\": 3}\n",
    "class CellDataset(Dataset):\n",
    "    def __init__(self, image_dir, df, transforms=None, resize=False):\n",
    "        self.transforms = transforms\n",
    "        self.image_dir = image_dir\n",
    "        self.df = df\n",
    "        \n",
    "        self.should_resize = resize is not False\n",
    "        if self.should_resize:\n",
    "            self.height = int(HEIGHT * resize)\n",
    "            self.width = int(WIDTH * resize)\n",
    "            print(\"image size used:\", self.height, self.width)\n",
    "        else:\n",
    "            self.height = HEIGHT\n",
    "            self.width = WIDTH\n",
    "        \n",
    "        self.image_info = collections.defaultdict(dict)\n",
    "        temp_df = self.df.groupby([\"id\", \"cell_type\"])['annotation'].agg(lambda x: list(x)).reset_index()\n",
    "        for index, row in temp_df.iterrows():\n",
    "            self.image_info[index] = {\n",
    "                    'image_id': row['id'],\n",
    "                    'image_path': os.path.join(self.image_dir, row['id'] + '.png'),\n",
    "                    'annotations': list(row[\"annotation\"]),\n",
    "                    'cell_type': cell_type_dict[row[\"cell_type\"]]\n",
    "                    }\n",
    "    def get_box(self, a_mask):\n",
    "        ''' Get the bounding box of a given mask '''\n",
    "        pos = np.where(a_mask)\n",
    "        xmin = np.min(pos[1])\n",
    "        xmax = np.max(pos[1])\n",
    "        ymin = np.min(pos[0])\n",
    "        ymax = np.max(pos[0])\n",
    "        return [xmin, ymin, xmax, ymax]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ''' Get the image and the target'''\n",
    "        \n",
    "        img_path = self.image_info[idx][\"image_path\"]\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        \n",
    "        if self.should_resize:\n",
    "            img = cv2.resize(img, (self.width, self.height))\n",
    "\n",
    "        info = self.image_info[idx]\n",
    "\n",
    "        n_objects = len(info['annotations'])\n",
    "        masks = np.zeros((len(info['annotations']), self.height, self.width), dtype=np.uint8)\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for i, annotation in enumerate(info['annotations']):\n",
    "            a_mask = rle_decode(annotation, (HEIGHT, WIDTH))\n",
    "            \n",
    "            if self.should_resize:\n",
    "                a_mask = cv2.resize(a_mask, (self.width, self.height))\n",
    "            \n",
    "            a_mask = np.array(a_mask) > 0\n",
    "            masks[i, :, :] = a_mask\n",
    "            \n",
    "            boxes.append(self.get_box(a_mask))\n",
    "                # labels\n",
    "        labels = [int(info[\"cell_type\"]) for _ in range(n_objects)]\n",
    "        #labels = [1 for _ in range(n_objects)]\n",
    "        \n",
    "        \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((n_objects,), dtype=torch.int64)\n",
    "\n",
    "        # This is the required target for the Mask R-CNN\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'masks': masks,\n",
    "            'image_id': image_id,\n",
    "            'area': area,\n",
    "            'iscrowd': iscrowd\n",
    "        }\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>annotation</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>plate_time</th>\n",
       "      <th>sample_date</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>elapsed_timedelta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0030fd0e6378</td>\n",
       "      <td>118145 6 118849 7 119553 8 120257 8 120961 9 1...</td>\n",
       "      <td>704</td>\n",
       "      <td>520</td>\n",
       "      <td>shsy5y</td>\n",
       "      <td>11h30m00s</td>\n",
       "      <td>2019-06-16</td>\n",
       "      <td>shsy5y[diff]_E10-4_Vessel-714_Ph_3</td>\n",
       "      <td>0 days 11:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0030fd0e6378</td>\n",
       "      <td>189036 1 189739 3 190441 6 191144 7 191848 8 1...</td>\n",
       "      <td>704</td>\n",
       "      <td>520</td>\n",
       "      <td>shsy5y</td>\n",
       "      <td>11h30m00s</td>\n",
       "      <td>2019-06-16</td>\n",
       "      <td>shsy5y[diff]_E10-4_Vessel-714_Ph_3</td>\n",
       "      <td>0 days 11:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0030fd0e6378</td>\n",
       "      <td>173567 3 174270 5 174974 5 175678 6 176382 7 1...</td>\n",
       "      <td>704</td>\n",
       "      <td>520</td>\n",
       "      <td>shsy5y</td>\n",
       "      <td>11h30m00s</td>\n",
       "      <td>2019-06-16</td>\n",
       "      <td>shsy5y[diff]_E10-4_Vessel-714_Ph_3</td>\n",
       "      <td>0 days 11:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0030fd0e6378</td>\n",
       "      <td>196723 4 197427 6 198130 7 198834 8 199538 8 2...</td>\n",
       "      <td>704</td>\n",
       "      <td>520</td>\n",
       "      <td>shsy5y</td>\n",
       "      <td>11h30m00s</td>\n",
       "      <td>2019-06-16</td>\n",
       "      <td>shsy5y[diff]_E10-4_Vessel-714_Ph_3</td>\n",
       "      <td>0 days 11:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0030fd0e6378</td>\n",
       "      <td>167818 3 168522 5 169225 7 169928 8 170632 9 1...</td>\n",
       "      <td>704</td>\n",
       "      <td>520</td>\n",
       "      <td>shsy5y</td>\n",
       "      <td>11h30m00s</td>\n",
       "      <td>2019-06-16</td>\n",
       "      <td>shsy5y[diff]_E10-4_Vessel-714_Ph_3</td>\n",
       "      <td>0 days 11:30:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                         annotation  width  \\\n",
       "0  0030fd0e6378  118145 6 118849 7 119553 8 120257 8 120961 9 1...    704   \n",
       "1  0030fd0e6378  189036 1 189739 3 190441 6 191144 7 191848 8 1...    704   \n",
       "2  0030fd0e6378  173567 3 174270 5 174974 5 175678 6 176382 7 1...    704   \n",
       "3  0030fd0e6378  196723 4 197427 6 198130 7 198834 8 199538 8 2...    704   \n",
       "4  0030fd0e6378  167818 3 168522 5 169225 7 169928 8 170632 9 1...    704   \n",
       "\n",
       "   height cell_type plate_time sample_date  \\\n",
       "0     520    shsy5y  11h30m00s  2019-06-16   \n",
       "1     520    shsy5y  11h30m00s  2019-06-16   \n",
       "2     520    shsy5y  11h30m00s  2019-06-16   \n",
       "3     520    shsy5y  11h30m00s  2019-06-16   \n",
       "4     520    shsy5y  11h30m00s  2019-06-16   \n",
       "\n",
       "                            sample_id elapsed_timedelta  \n",
       "0  shsy5y[diff]_E10-4_Vessel-714_Ph_3   0 days 11:30:00  \n",
       "1  shsy5y[diff]_E10-4_Vessel-714_Ph_3   0 days 11:30:00  \n",
       "2  shsy5y[diff]_E10-4_Vessel-714_Ph_3   0 days 11:30:00  \n",
       "3  shsy5y[diff]_E10-4_Vessel-714_Ph_3   0 days 11:30:00  \n",
       "4  shsy5y[diff]_E10-4_Vessel-714_Ph_3   0 days 11:30:00  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_base = pd.read_csv(TRAIN_CSV)\n",
    "df_base.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>annotation</th>\n",
       "      <th>quantiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a7b1db2a42fc</td>\n",
       "      <td>astro</td>\n",
       "      <td>594</td>\n",
       "      <td>(105.0, 594.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>903d94c69354</td>\n",
       "      <td>astro</td>\n",
       "      <td>351</td>\n",
       "      <td>(105.0, 594.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2c2cb870da85</td>\n",
       "      <td>astro</td>\n",
       "      <td>174</td>\n",
       "      <td>(105.0, 594.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>1ea4e44e5497</td>\n",
       "      <td>astro</td>\n",
       "      <td>164</td>\n",
       "      <td>(105.0, 594.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>a75cdb426a8e</td>\n",
       "      <td>astro</td>\n",
       "      <td>163</td>\n",
       "      <td>(105.0, 594.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id cell_type  annotation       quantiles\n",
       "5    a7b1db2a42fc     astro         594  (105.0, 594.0]\n",
       "71   903d94c69354     astro         351  (105.0, 594.0]\n",
       "135  2c2cb870da85     astro         174  (105.0, 594.0]\n",
       "138  1ea4e44e5497     astro         164  (105.0, 594.0]\n",
       "139  a75cdb426a8e     astro         163  (105.0, 594.0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>annotation</th>\n",
       "      <th>quantiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>4425efbbacfc</td>\n",
       "      <td>cort</td>\n",
       "      <td>108</td>\n",
       "      <td>(43.0, 108.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>4b8dc9c901a6</td>\n",
       "      <td>cort</td>\n",
       "      <td>94</td>\n",
       "      <td>(43.0, 108.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>adfd16bee70c</td>\n",
       "      <td>cort</td>\n",
       "      <td>94</td>\n",
       "      <td>(43.0, 108.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>76ad9ac01e2d</td>\n",
       "      <td>cort</td>\n",
       "      <td>94</td>\n",
       "      <td>(43.0, 108.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>44a154410273</td>\n",
       "      <td>cort</td>\n",
       "      <td>89</td>\n",
       "      <td>(43.0, 108.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id cell_type  annotation      quantiles\n",
       "165  4425efbbacfc      cort         108  (43.0, 108.0]\n",
       "185  4b8dc9c901a6      cort          94  (43.0, 108.0]\n",
       "186  adfd16bee70c      cort          94  (43.0, 108.0]\n",
       "188  76ad9ac01e2d      cort          94  (43.0, 108.0]\n",
       "196  44a154410273      cort          89  (43.0, 108.0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cell_type</th>\n",
       "      <th>annotation</th>\n",
       "      <th>quantiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c4121689002f</td>\n",
       "      <td>shsy5y</td>\n",
       "      <td>790</td>\n",
       "      <td>(447.8, 790.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d164e96bb7a9</td>\n",
       "      <td>shsy5y</td>\n",
       "      <td>782</td>\n",
       "      <td>(447.8, 790.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e748ac1c469b</td>\n",
       "      <td>shsy5y</td>\n",
       "      <td>703</td>\n",
       "      <td>(447.8, 790.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aff8fb4fc364</td>\n",
       "      <td>shsy5y</td>\n",
       "      <td>609</td>\n",
       "      <td>(447.8, 790.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e8ae919aa92e</td>\n",
       "      <td>shsy5y</td>\n",
       "      <td>605</td>\n",
       "      <td>(447.8, 790.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id cell_type  annotation       quantiles\n",
       "0  c4121689002f    shsy5y         790  (447.8, 790.0]\n",
       "1  d164e96bb7a9    shsy5y         782  (447.8, 790.0]\n",
       "2  e748ac1c469b    shsy5y         703  (447.8, 790.0]\n",
       "3  aff8fb4fc364    shsy5y         609  (447.8, 790.0]\n",
       "4  e8ae919aa92e    shsy5y         605  (447.8, 790.0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_images = df_base.groupby([\"id\", \"cell_type\"]).agg({'annotation': 'count'}).sort_values(\"annotation\", ascending=False).reset_index()\n",
    "\n",
    "for ct in cell_type_dict:\n",
    "    ctdf = df_images[df_images[\"cell_type\"]==ct].copy()\n",
    "    if len(ctdf)>0:\n",
    "        ctdf['quantiles'] = pd.qcut(ctdf['annotation'], 5)\n",
    "        display(ctdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cell_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>astro</th>\n",
       "      <td>131</td>\n",
       "      <td>80</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>73</td>\n",
       "      <td>100</td>\n",
       "      <td>594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cort</th>\n",
       "      <td>320</td>\n",
       "      <td>33</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>30</td>\n",
       "      <td>39</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shsy5y</th>\n",
       "      <td>155</td>\n",
       "      <td>337</td>\n",
       "      <td>149</td>\n",
       "      <td>49</td>\n",
       "      <td>235</td>\n",
       "      <td>324</td>\n",
       "      <td>429</td>\n",
       "      <td>790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           count  mean  std  min  25%  50%  75%  max\n",
       "cell_type                                           \n",
       "astro        131    80   64    5   50   73  100  594\n",
       "cort         320    33   16    4   23   30   39  108\n",
       "shsy5y       155   337  149   49  235  324  429  790"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_images.groupby(\"cell_type\").annotation.describe().astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       annotation\n",
       "count         606\n",
       "mean          121\n",
       "std           152\n",
       "min             4\n",
       "25%            28\n",
       "50%            46\n",
       "75%           140\n",
       "max           790"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_images[['annotation']].describe().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images_train, df_images_val = train_test_split(df_images, stratify=df_images['cell_type'], \n",
    "                                                  test_size=PCT_IMAGES_VALIDATION,\n",
    "                                                  random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images in train set:           560\n",
      "Annotations in train set:      68600\n",
      "Images in validation set:      46\n",
      "Annotations in validation set: 4985\n"
     ]
    }
   ],
   "source": [
    "df_train = df_base[df_base['id'].isin(df_images_train['id'])]\n",
    "df_val = df_base[df_base['id'].isin(df_images_val['id'])]\n",
    "print(f\"Images in train set:           {len(df_images_train)}\")\n",
    "print(f\"Annotations in train set:      {len(df_train)}\")\n",
    "print(f\"Images in validation set:      {len(df_images_val)}\")\n",
    "print(f\"Annotations in validation set: {len(df_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = CellDataset(TRAIN, df_train, resize=resize_factor, transforms=get_transform(train=True))\n",
    "dl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True,\n",
    "                      num_workers=2, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "ds_val = CellDataset(TRAIN, df_val, resize=resize_factor, transforms=get_transform(train=False))\n",
    "dl_val = DataLoader(ds_val, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True,\n",
    "                    num_workers=2, collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "setup the training harnessusing torchvision model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_classes, model_chkpt=None):\n",
    "    # This is just a dummy value for the classification head\n",
    "    \n",
    "    if NORMALIZE:\n",
    "        model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True,\n",
    "                                                                   box_detections_per_img=BOX_DETECTIONS_PER_IMG,\n",
    "                                                                   image_mean=RESNET_MEAN,\n",
    "                                                                   image_std=RESNET_STD)\n",
    "    else:\n",
    "        model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True,\n",
    "                                                                   box_detections_per_img=BOX_DETECTIONS_PER_IMG)\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes+1)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes+1)\n",
    "    \n",
    "    if model_chkpt:\n",
    "        model.load_state_dict(torch.load(model_chkpt, map_location=DEVICE))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michael/anaconda3/envs/pytorch/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/michael/anaconda3/envs/pytorch/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MaskRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MaskRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n",
       "    )\n",
       "    (mask_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(14, 14), sampling_ratio=2)\n",
       "    (mask_head): MaskRCNNHeads(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Conv2dNormActivation(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (mask_predictor): MaskRCNNPredictor(\n",
       "      (conv5_mask): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (mask_fcn_logits): Conv2d(256, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model(len(cell_type_dict))\n",
    "model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "n_batches, n_batches_val = len(dl_train), len(dl_val)\n",
    "\n",
    "validation_mask_losses = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1 of 30\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 392.00 MiB (GPU 0; 3.82 GiB total capacity; 2.16 GiB already allocated; 250.62 MiB free; 2.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4408/3624401382.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[operator]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"targets should not be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatched_gt_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_targets_to_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m             \u001b[0mregression_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_coder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatched_gt_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             loss_objectness, loss_rpn_box_reg = self.compute_loss(\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36massign_targets_to_anchors\u001b[0;34m(self, anchors, targets)\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0mlabels_per_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors_per_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m                 \u001b[0mmatch_quality_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors_per_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m                 \u001b[0mmatched_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproposal_matcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch_quality_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;31m# get the targets corresponding GT for each proposal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torchvision/ops/boxes.py\u001b[0m in \u001b[0;36mbox_iou\u001b[0;34m(boxes1, boxes2)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_iou\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m     \u001b[0minter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_box_inter_union\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0miou\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minter\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0munion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0miou\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torchvision/ops/boxes.py\u001b[0m in \u001b[0;36m_box_inter_union\u001b[0;34m(boxes1, boxes2)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0mrb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [N,M,2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0mwh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrb\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [N,M,2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m     \u001b[0minter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mwh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# [N,M]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 392.00 MiB (GPU 0; 3.82 GiB total capacity; 2.16 GiB already allocated; 250.62 MiB free; 2.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    print(f\"Starting epoch {epoch} of {NUM_EPOCHS}\")\n",
    "\n",
    "    time_start = time.time()\n",
    "    loss_accum = 0.0\n",
    "    loss_mask_accum = 0.0\n",
    "    loss_classifier_accum = 0.0\n",
    "    for batch_idx, (images, targets) in enumerate(dl_train, 1):\n",
    "    \n",
    "        # Predict\n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "         # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Logging\n",
    "        loss_mask = loss_dict['loss_mask'].item()\n",
    "        loss_accum += loss.item()\n",
    "        loss_mask_accum += loss_mask\n",
    "        loss_classifier_accum += loss_dict['loss_classifier'].item()\n",
    "        \n",
    "        if batch_idx % 500 == 0:\n",
    "            print(f\"    [Batch {batch_idx:3d} / {n_batches:3d}] Batch train loss: {loss.item():7.3f}. Mask-only loss: {loss_mask:7.3f}.\")\n",
    "        \n",
    "    if USE_SCHEDULER:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    # Train losses\n",
    "    train_loss = loss_accum / n_batches\n",
    "    train_loss_mask = loss_mask_accum / n_batches\n",
    "    train_loss_classifier = loss_classifier_accum / n_batches\n",
    "\n",
    "    # Validation\n",
    "    val_loss_accum = 0\n",
    "    val_loss_mask_accum = 0\n",
    "    val_loss_classifier_accum = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, targets) in enumerate(dl_val, 1):\n",
    "            images = list(image.to(DEVICE) for image in images)\n",
    "            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            val_loss_dict = model(images, targets)\n",
    "            val_batch_loss = sum(loss for loss in val_loss_dict.values())\n",
    "            val_loss_accum += val_batch_loss.item()\n",
    "            val_loss_mask_accum += val_loss_dict['loss_mask'].item()\n",
    "            val_loss_classifier_accum += val_loss_dict['loss_classifier'].item()\n",
    "\n",
    "    # Validation losses\n",
    "    val_loss = val_loss_accum / n_batches_val\n",
    "    val_loss_mask = val_loss_mask_accum / n_batches_val\n",
    "    val_loss_classifier = val_loss_classifier_accum / n_batches_val\n",
    "    elapsed = time.time() - time_start\n",
    "\n",
    "    validation_mask_losses.append(val_loss_mask)\n",
    "\n",
    "    torch.save(model.state_dict(), f\"pytorch_model-e{epoch}.bin\")\n",
    "    prefix = f\"[Epoch {epoch:2d} / {NUM_EPOCHS:2d}]\"\n",
    "    print(prefix)\n",
    "    print(f\"{prefix} Train mask-only loss: {train_loss_mask:7.3f}, classifier loss {train_loss_classifier:7.3f}\")\n",
    "    print(f\"{prefix} Val mask-only loss  : {val_loss_mask:7.3f}, classifier loss {val_loss_classifier:7.3f}\")\n",
    "    print(prefix)\n",
    "    print(f\"{prefix} Train loss: {train_loss:7.3f}. Val loss: {val_loss:7.3f} [{elapsed:.0f} secs]\")\n",
    "    print(prefix)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_train_sample(model, ds_train, sample_index):\n",
    "    \n",
    "    img, targets = ds_train[sample_index]\n",
    "    #print(img.shape)\n",
    "    l = np.unique(targets[\"labels\"])\n",
    "    ig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20,60), facecolor=\"#fefefe\")\n",
    "    ax[0].imshow(img.numpy().transpose((1,2,0)))\n",
    "    ax[0].set_title(f\"cell type {l}\")\n",
    "    ax[0].axis(\"off\")\n",
    "    \n",
    "    masks = combine_masks(targets['masks'], 0.5)\n",
    "    #plt.imshow(img.numpy().transpose((1,2,0)))\n",
    "    ax[1].imshow(masks)\n",
    "    ax[1].set_title(f\"Ground truth, {len(targets['masks'])} cells\")\n",
    "    ax[1].axis(\"off\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model([img.to(DEVICE)])[0]\n",
    "    l = pd.Series(preds['labels'].cpu().numpy()).value_counts()\n",
    "    lstr = \"\"\n",
    "    for i in l.index:\n",
    "        lstr += f\"{l[i]}x{i} \"\n",
    "    #print(l, l.sort_values().index[-1])\n",
    "    #plt.imshow(img.cpu().numpy().transpose((1,2,0)))\n",
    "    mask_threshold = mask_threshold_dict[l.sort_values().index[-1]]\n",
    "    #print(mask_threshold)\n",
    "    pred_masks = combine_masks(get_filtered_masks(preds), mask_threshold)\n",
    "    ax[2].imshow(pred_masks)\n",
    "    ax[2].set_title(f\"Predictions, labels: {lstr}\")\n",
    "    ax[2].axis(\"off\")\n",
    "    plt.show() \n",
    "    \n",
    "    #print(masks.shape, pred_masks.shape)\n",
    "    score = iou_map([masks],[pred_masks])\n",
    "    print(\"Score:\", score)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_train_sample(model, ds_train, 102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_train_sample(model, ds_train, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_train_sample(model, ds_train, 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d8596ea138c5dd1d107a51e271cce4498866561bde2c931f6384b91ad879151c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
